---
title: "DeepEPÂ : la bibliothÃ¨que de communication ultime pour vos modÃ¨les MoE"
summary: "DÃ©couvrez comment DeepEP propulse lâ€™entraÃ®nement et lâ€™infÃ©rence des architectures Mixtureâ€‘ofâ€‘Experts grÃ¢ce Ã  des kernels allâ€‘toâ€‘all ultraâ€‘performants, une prise en charge FP8 et un chevauchement communicationâ€‘calcul sans occuper les SM."
image: "/images/blog/deepep/deepep-cover.jpg"
publishedAt: "2025-07-24"
tag: "DeepEP"
author: "Jensen Huang"
---

## Pourquoi DeepEPâ€¯?

---

Les modÃ¨les **Mixtureâ€‘ofâ€‘Experts (MoE)** ont dÃ©montrÃ© un rapport capacitÃ©/performance exceptionnel, mais leur _talon dâ€™Achille_ reste la **communication allâ€‘toâ€‘all** (dispatch/combine) entre GPU.  
DeepEP, dÃ©veloppÃ© par lâ€™Ã©quipe **DeepSeekâ€‘AI**, relÃ¨ve ce dÃ©fi grÃ¢ce Ã â€¯:

1. **Kernels haut dÃ©bit** optimisÃ©s NVLinkâ€¯â†”â€¯RDMA, idÃ©aux pour lâ€™entraÃ®nement et le _prefilling_ dâ€™infÃ©rence.
2. **Kernels basse latence** pure RDMA pour le dÃ©codage tokenâ€‘parâ€‘token.
3. Un **hook** de chevauchement commâ€‘calc **sans monopoliser de SM**.
4. Un design Ã©volutifâ€¯: SM80 (Ampere) et SM90 (Hopper)â€¯; FP8 dispatch / BF16 combine.

---

## Performances clÃ©s

|   Type    | Dispatch #EP | Bottleneck bandwidth | Combine #EP | Bottleneck bandwidth |
| :-------: | :----------: | :------------------: | :---------: | :------------------: |
| Intranode |      8       |  153 GB/s (NVLink)   |      8      |  158 GB/s (NVLink)   |
| Internode |      16      |    43 GB/s (RDMA)    |     16      |    43 GB/s (RDMA)    |
| Internode |      32      |    58 GB/s (RDMA)    |     32      |    57 GB/s (RDMA)    |
| Internode |      64      |    51 GB/s (RDMA)    |     64      |    50 GB/s (RDMA)    |

> _Ã€ noterÂ :_ grÃ¢ce aux optimisations rÃ©seau (avrilâ€¯2025) en collaboration avec Tencent, les perfs ont gagnÃ© **jusquâ€™Ã â€¯30â€¯%** ğŸ”¥.

---

## Installation express

### PrÃ© requis

- GPU AmpÃ¨re (SM80), Hopper (SM90) ou autres architectures avec prise en charge de lâ€™ISA PTX SM90
- PythonÂ 3.8 et versions supÃ©rieures
- Version de CUDAÂ :
  - CUDAÂ 11.0 et ultÃ©rieure pour les GPU SM80
  - CUDAÂ 12.3 et ultÃ©rieure pour les GPU SM90
- PyTorchÂ 2.1 et versions supÃ©rieures
- NVLink pour la communication intranÅ“ud
- RÃ©seau RDMA pour la communication internÅ“ud

### TÃ©lÃ©chargement et installation de la dÃ©pendance NVSHMEM

DeepEP dÃ©pend Ã©galement de NVSHMEM. Veuillez consulter notre [Guide dâ€™installation de NVSHMEM](third-party/README.md) pour les instructions.

Cloner le dÃ©pÃ´t :

<CodeBlock
  compact
  marginBottom="16"
  codeInstances={[
    {
      code: "git clone https://github.com/deepseek-ai/DeepEP",
      language: "bash",
    },
  ]}
/>

Compiler NVSHMEM (requis pour lâ€™interâ€‘nÅ“uds):

<CodeBlock
  compact
  marginBottom="16"
  codeInstances={[
    {
      code: "NVSHMEM_DIR=/chemin/vers/nvshmem python setup.py install",
      language: "bash",
    },
  ]}
/>

## Premier test

<CodeBlock
  compact
  marginBottom="16"
  codeInstances={[
    {
      code: `python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py`,
      language: "bash",
    },
  ]}
/>
### Exemple 

<CodeBlock
  marginBottom="16"
  highlight="1-80"
  codeInstances={[
    {
      code: `import torch
import torch.distributed as dist
from typing import List, Tuple, Optional, Union

from deep_ep import Buffer, EventOverlap

# Communication buffer (will allocate at runtime)

\_buffer: Optional[Buffer] = None

# Set the number of SMs to use

# NOTES: this is une variable statique

Buffer.set_num_sms(24)

# You may call this function at the framework initialization

def get_buffer(group: dist.ProcessGroup, hidden_bytes: int) -> Buffer:
global \_buffer

    # NOTES: you may aussi remplacer 'get_*_config' par vos rÃ©sultats auto-tunÃ©s
    num_nvl_bytes, num_rdma_bytes = 0, 0
    for config in (Buffer.get_dispatch_config(group.size()), Buffer.get_combine_config(group.size())):
        num_nvl_bytes = max(config.get_nvl_buffer_size_hint(hidden_bytes, group.size()), num_nvl_bytes)
        num_rdma_bytes = max(config.get_rdma_buffer_size_hint(hidden_bytes, group.size()), num_rdma_bytes)

    # Allocate a buffer if pas dÃ©jÃ  crÃ©Ã© ou pas assez gros
    if _buffer is None or _buffer.group != group or _buffer.num_nvl_bytes < num_nvl_bytes or _buffer.num_rdma_bytes < num_rdma_bytes:
        _buffer = Buffer(group, num_nvl_bytes, num_rdma_bytes)
    return _buffer

def get_hidden_bytes(x: torch.Tensor) -> int:
t = x[0] if isinstance(x, tuple) else x
return t.size(1) \* max(t.element_size(), 2)

def dispatch_forward(x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
topk_idx: torch.Tensor, topk_weights: torch.Tensor,
num_experts: int, previous_event: Optional[EventOverlap] = None) -> \
 Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], torch.Tensor, torch.Tensor, List, Tuple, EventOverlap]:
global \_buffer

    # Calculate layout before actual dispatch
    num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert, is_token_in_rank, previous_event = \
        _buffer.get_dispatch_layout(topk_idx, num_experts,
                                    previous_event=previous_event, async_finish=True,
                                    allocate_on_comm_stream=previous_event is not None)
    # Do MoE dispatch
    recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event = \
        _buffer.dispatch(x, topk_idx=topk_idx, topk_weights=topk_weights,
                         num_tokens_per_rank=num_tokens_per_rank, num_tokens_per_rdma_rank=num_rdma_bytes,
                         is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_experts,
                         previous_event=previous_event, async_finish=True,
                         allocate_on_comm_stream=True)
    return recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event

def dispatch_backward(grad_recv_x: torch.Tensor, grad_recv_topk_weights: torch.Tensor, handle: Tuple) -> \
 Tuple[torch.Tensor, torch.Tensor, EventOverlap]:
global \_buffer

    combined_grad_x, combined_grad_recv_topk_weights, event = \
        _buffer.combine(grad_recv_x, handle, topk_weights=grad_recv_topk_weights, async_finish=True)
    return combined_grad_x, combined_grad_recv_topk_weights, event

def combine_forward(x: torch.Tensor, handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
 Tuple[torch.Tensor, EventOverlap]:
global \_buffer

    combined_x, _, event = _buffer.combine(x, handle, async_finish=True, previous_event=previous_event,
                                           allocate_on_comm_stream=previous_event is not None)
    return combined_x, event

def combine_backward(grad_combined_x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
 Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], EventOverlap]:
global \_buffer

    grad_x, _, _, _, _, event = _buffer.dispatch(grad_combined_x, handle=handle, async_finish=True,
                                                 previous_event=previous_event,
                                                 allocate_on_comm_stream=previous_event is not None)
    return grad_x, event

`,
language: "python",
label: "deep_ep_example.py",
},
]}
/>
